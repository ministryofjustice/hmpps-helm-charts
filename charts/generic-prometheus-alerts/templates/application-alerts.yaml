{{- $targetNamespace := .Release.Namespace }}
{{- $targetNamespaceUnderscore := .Release.Namespace | replace "-" "_" }}
{{- $targetApplication := required "A value for targetApplication must be set" .Values.targetApplication }}
{{- $targetPodRegex := printf "%s(-[a-z0-9]{4,10}-[a-z0-9]{5})" .Values.targetApplication }}
{{- $targetPod := default $targetPodRegex .Values.podTargetOverride }}
{{- $targetDeployment := default .Values.targetApplication .Values.deploymentTargetOverride }}
{{- $targetHpa := default .Values.targetApplication .Values.hpaTargetOverride }}
{{- $targetApplicationBusinessHours := printf "and ON() %s:business_hours" .Values.targetApplication | replace "-" "_" }}
{{- $businessOrAllHoursExpression := ternary $targetApplicationBusinessHours "" .Values.businessHoursOnly}}
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ $targetApplication }}-app
  labels:
    {{- include "generic-prometheus-alerts.labels" . | nindent 4 }}
spec:
  groups:
    - name: {{ $targetApplication }}-app
      rules:
        - alert: KubePodCrashLooping
          annotations:
            summary: Pod is crash looping.
            message: Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} ({{`{{`}} $labels.container {{`}}`}}) is restarting {{`{{`}} printf "%.2f" $value {{`}}`}} times every 5 minutes.
            runbook_url: {{ .Values.runbookUrl }}application-pod-crashlooping
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |
            rate(kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", pod=~"{{ $targetPod }}"}[10m]) * 60 * 5 > 0
            {{ $businessOrAllHoursExpression }}
          for: 15m
          labels:
            {{- include "generic-prometheus-alerts.ruleLabels" $ | nindent 12 }}

        - alert: KubePodNotReady
          annotations:
            summary: Pod in a non-ready state.
            message: Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} has been in a non-ready state for longer than 15 minutes.
            runbook_url: {{ .Values.runbookUrl }}application-pod-notready
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |-
            sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", pod=~"{{ $targetPod }}", phase=~"Pending|Unknown"}) > 0
            {{ $businessOrAllHoursExpression }}
          for: 15m
          labels:
            {{- include "generic-prometheus-alerts.ruleLabels" $ | nindent 12 }}

        - alert: KubeDeploymentGenerationMismatch
          annotations:
            summary: Deployment generation mismatch.
            message: Deployment generation for {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.deployment {{`}}`}} does not match, this indicates that the Deployment has failed but has not been rolled back.
            runbook_url: {{ .Values.runbookUrl }}application-deployment-generation-mismatch
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |-
            (kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", deployment=~"{{ $targetDeployment }}"}
              !=
            kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", deployment=~"{{ $targetDeployment }}"})
            {{ $businessOrAllHoursExpression }}
          for: 15m
          labels:
            {{- include "generic-prometheus-alerts.ruleLabels" $ | nindent 12 }}

        - alert: KubeDeploymentReplicasMismatch
          annotations:
            summary: Deployment has not matched the expected number of replicas.
            message: Deployment {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.deployment {{`}}`}} has not matched the expected number of replicas for longer 15 minutes.
            runbook_url: {{ .Values.runbookUrl }}application-deployment-replicas-mismatch
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |-
            (kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", deployment=~"{{ $targetDeployment }}"}
              !=
            kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", deployment=~"{{ $targetDeployment }}"})
            {{ $businessOrAllHoursExpression }}
          for: 15m
          labels:
            {{- include "generic-prometheus-alerts.ruleLabels" $ | nindent 12 }}

        - alert: KubeContainerWaiting
          annotations:
            summary: Pod container waiting.
            message: Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} container {{`{{`}} $labels.container {{`}}`}} has been in waiting state for longer than 1 hour.
            runbook_url: {{ .Values.runbookUrl }}application-container-waiting
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |
            sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", pod=~"{{ $targetPod }}"}) > 0
            {{ $businessOrAllHoursExpression }}
          for: 1h
          labels:
            {{- include "generic-prometheus-alerts.ruleLabels" $ | nindent 12 }}

        - record: job:kube_job_status_start_time_{{ $targetNamespaceUnderscore }}:max
          expr: |
            label_replace(
              label_replace(
                max(
                  kube_job_status_start_time{namespace=~"{{ $targetNamespace }}"}
                  * ON(job_name,namespace) GROUP_RIGHT()
                  kube_job_owner{owner_name!="", namespace=~"{{ $targetNamespace }}"}
                )
                BY (job_name, owner_name, namespace)
                == ON(owner_name) GROUP_LEFT()
                max(
                  kube_job_status_start_time{namespace=~"{{ $targetNamespace }}"}
                  * ON(job_name,namespace) GROUP_RIGHT()
                  kube_job_owner{owner_name!="", namespace=~"{{ $targetNamespace }}"}
                )
                BY (owner_name),
              "job", "$1", "job_name", "(.+)"),
            "cronjob", "$1", "owner_name", "(.+)")
            {{ $businessOrAllHoursExpression }}
        - record: job:kube_job_status_failed_{{ $targetNamespaceUnderscore }}:sum
          expr: |
            clamp_max(job:kube_job_status_start_time_{{ $targetNamespaceUnderscore }}:max,1)
              * ON(job) GROUP_LEFT()
              label_replace(
                label_replace(
                  (kube_job_status_failed{namespace=~"{{ $targetNamespace }}"} != 0),
                  "job", "$1", "job_name", "(.+)"),
                "cronjob", "$1", "owner_name", "(.+)")
            {{ $businessOrAllHoursExpression }}
        - alert: CronJobStatusFailed
          annotations:
            summary: CronJob is failing.
            message: CronJob {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.cronjob {{`}}`}} is failing.
            runbook_url: {{ .Values.runbookUrl }}application-cronjob-failed
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |
            (count_over_time(job:kube_job_status_failed_{{ $targetNamespaceUnderscore }}:sum[{{ .Values.applicationCronJobStatusFailedWindowMinutes }}m])
            * ON(cronjob,namespace) GROUP_LEFT()
            (kube_cronjob_spec_suspend == 0))
            {{ $businessOrAllHoursExpression }}
          labels:
            {{- include "generic-prometheus-alerts.ruleLabels" $ | nindent 12 }}

        - alert: KubeHpaReplicasMismatch
          annotations:
            summary: HPA has not matched desired number of replicas.
            message: HPA {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.hpa {{`}}`}} has not matched the desired number of replicas for longer than 15 minutes.
            runbook_url: {{ .Values.runbookUrl }}application-hpa-replicas-mismatch
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |
            ((kube_hpa_status_desired_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", hpa=~"{{ $targetHpa }}"}
              !=
            kube_hpa_status_current_replicas{job="kube-state-metrics"})
              and
            (kube_hpa_status_current_replicas{job="kube-state-metrics"}
              >
            kube_hpa_spec_min_replicas{job="kube-state-metrics"})
              and
            (kube_hpa_status_current_replicas{job="kube-state-metrics"}
              <
            kube_hpa_spec_max_replicas{job="kube-state-metrics"})
              and
            changes(kube_hpa_status_current_replicas{job="kube-state-metrics"}[15m]) == 0)
            {{ $businessOrAllHoursExpression }}
          for: 15m
          labels:
            {{- include "generic-prometheus-alerts.ruleLabels" $ | nindent 12 }}

        - alert: KubeHpaMaxedOut
          annotations:
            summary: HPA is running at max replicas
            message: HPA {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.hpa {{`}}`}} has been running at max replicas for longer than 15 minutes.
            runbook_url: {{ .Values.runbookUrl }}application-hpa-maxed-out
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |
            (kube_hpa_status_current_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", hpa=~"{{ $targetHpa }}"}
              ==
            kube_hpa_spec_max_replicas{job="kube-state-metrics"})
            {{ $businessOrAllHoursExpression }}
          for: 15m
          labels:
            {{- include "generic-prometheus-alerts.ruleLabels" $ | nindent 12 }}

        - alert: KubeQuotaExceeded
          annotations:
            summary: Namespace quota has exceeded the limits.
            message: Namespace {{`{{`}} $labels.namespace {{`}}`}} is using {{`{{`}} printf "%0.0f" $value {{`}}`}}% of its {{`{{`}} $labels.resource {{`}}`}} quota.
            runbook_url: {{ .Values.runbookUrl }}application-quota-exceeded
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |-
            (100 * kube_resourcequota{namespace=~"{{ $targetNamespace }}", job="kube-state-metrics", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{namespace=~"{{ $targetNamespace }}", job="kube-state-metrics", type="hard"} > 0)
              > 90)
            {{ $businessOrAllHoursExpression }}
          for: 15m
          labels:
            {{- include "generic-prometheus-alerts.ruleLabels" $ | nindent 12 }}

        - alert: KubeContainerOOMKilled
          annotations:
            summary: Kubernetes container OOM killed.
            message: "Container {{`{{`}} $labels.container {{`}}`}} in pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} has been OOM Killed (out of memory) {{`{{`}} $value {{`}}`}} times in the last 10 minutes."
            runbook_url: {{ .Values.runbookUrl }}application-container-oom-killed
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |-
            ((kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", pod=~"{{ $targetPod }}"}
             - kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", pod=~"{{ $targetPod }}"} offset 10m >= 1)
              and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", pod=~"{{ $targetPod }}", reason="OOMKilled"}[10m]) == 1)
            {{ $businessOrAllHoursExpression }}
          for: 0m
          labels:
            {{- include "generic-prometheus-alerts.ruleLabels" $ | nindent 12 }}
