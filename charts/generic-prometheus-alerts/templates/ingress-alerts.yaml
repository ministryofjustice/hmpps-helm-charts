{{- if .Values.ingressAlertsEnabled -}}
{{- $targetNamespace := .Release.Namespace }}
{{- $targetApplication := required "A value for targetApplication must be set" .Values.targetApplication }}
{{- $defaultTargetIngress := print .Values.targetApplication ".*" }}
{{- $targetIngress := default $defaultTargetIngress .Values.ingressTargetOverride }}
{{- $modSecBlockingStatusCode := default "406" .Values.modSecBlockingStatusCodeOverride }}
{{- $targetApplicationBusinessHours := printf "and ON() %s:business_hours" .Values.targetApplication | replace "-" "_" }}
{{- $businessOrAllHoursExpression := ternary $targetApplicationBusinessHours "" .Values.businessHoursOnly}}
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ $targetApplication }}-ingress
  labels:
    {{- include "generic-prometheus-alerts.labels" . | nindent 4 }}
spec:
  groups:
    - name: {{ $targetApplication }}-ingress
      rules:
        - alert: 5xxErrorResponses
          annotations:
            message: Ingress {{`{{`}} $labels.exported_namespace {{`}}`}}/{{`{{`}} $labels.ingress {{`}}`}} is serving 5xx responses.
            runbook_url: {{ .Values.runbookUrl }}ingress-5xx-error-responses
            dashboard_url: {{ $.Values.grafanaUrl }}/d/{{ $.Values.dashboardId }}/kubernetes-ingress-traffic?{{ $.Values.orgId }}&var-namespace={{ $targetNamespace }}&var-ingress={{ $targetIngress }}&from=now-5m&to=now                                                                                                                                                                             
          expr: |-
            avg(rate(nginx_ingress_controller_requests{exported_namespace=~"{{ $targetNamespace }}", ingress=~"{{ $targetIngress }}", path!="/health", status=~"5.*"}[{{ .Values.ingress5xxErrorWindowMinutes }}m]) > 0) by (ingress, exported_namespace)
            {{ $businessOrAllHoursExpression }}
          for: 1m
          labels:
            severity: {{ .Values.alertSeverity }}

# ## Understanding the below PromQL expression with an example
# rate() - calculates the per-second rate of increase of a time series over a specified time range (ex: nginx_requests)
# series of http requests: 100, 150, 200, 3, 0, 40, 22, 1, 2, 11
# evaluation time: 5 minutes
# for each data point the rate of increase:
# 1. No rate calculation for the first data point (100)
# 2. Rate at t1: (150-100)/300 = 0.1667 requests per second
# 3. Rate at t1: (200-150)/300 = 0.1667 requests per second
# 4. Rate at t1: (3-200)/300 = -0.6567 requests per second
# 5. Rate at t1: (0-3)/300 = 0.-0.01 requests per second
# 6. Rate at t1: (40-0)/300 = 0.1333 requests per second
# 7. Rate at t1: (22-40)/300 = -0.06 requests per second
# 8. Rate at t1: (1-22)/300 = -0.0633 requests per second
# 9. Rate at t1: (2-1)/300 = 0.0033 requests per second
# 10.Rate at t1: (11-2)/300 = 0.03 requests per second
# average rate of increase: (0.1667 + 0.1667 - 0.6567 - 0.01 + 0.1333 - 0.06 - 0.0633 + 0.0033 + 0.03 )/ 9 = 0.0107 requests per second
# It states the average per-second rate of increase of the nginx requests.
# The below expression calculates the average rate of server errors (status codes starting with "5") for requests handles by the nginx ingress controller,
# - it excludes requests with the "/health" path
# - evaluates the expression over a 5 minute period (the last 5 mins)
# - groups the results by the 'ingress' and 'exported_namespace' labels
# - checks if the average rate of server errors is greater than 0.01 requests per second
# NOTE: '0.01' is configurable as and when needed

        # - alert: 5xxErrorResponsesOnHealthEndpoint
        #   annotations:
        #     message: High rate of 5xx errors detected on /health path in Ingress {{`{{`}} $labels.exported_namespace {{`}}`}}/{{`{{`}} $labels.ingress {{`}}`}}.
        #     runbook_url: {{ .Values.runbookUrl }}ingress-5xx-error-responses
        #     dashboard_url: {{ $.Values.grafanaUrl }}/d/{{ $.Values.dashboardId }}/kubernetes-ingress-traffic?{{ $.Values.orgId }}&var-namespace={{ $targetNamespace }}&var-ingress={{ $targetIngress }}&from=now-5m&to=now
        #   expr: |-
        #     avg(rate(nginx_ingress_controller_requests{exported_namespace=~"{{ $targetNamespace }}", ingress=~"{{ $targetIngress }}", path="/health", status=~"5.*"}[{{ .Values.ingress5xxErrorWindowMinutesHealthEndpoint }}m]) > 0.01) by (ingress, exported_namespace)
        #     {{ $businessOrAllHoursExpression }}
        #   for: 5m
        #   labels:
        #     severity: {{ .Values.alertSeverity }} 
        - alert: 5xxErrorResponsesOnHealthEndpoint
          annotations:
            message: High rate of 5xx errors detected on /health path in Ingress {{`{{`}} $labels.exported_namespace {{`}}`}}/{{`{{`}} $labels.ingress {{`}}`}}.
            runbook_url: {{ .Values.runbookUrl }}ingress-5xx-error-responses
            dashboard_url: {{ $.Values.grafanaUrl }}/d/{{ $.Values.dashboardId }}/kubernetes-ingress-traffic?{{ $.Values.orgId }}&var-namespace={{ $targetNamespace }}&var-ingress={{ $targetIngress }}&from=now-5m&to=now
          expr: |-
            sum(increase(nginx_ingress_controller_requests{exported_namespace=~"{{ $targetNamespace }}", ingress=~"{{ $targetIngress }}", path="/health", status=~"5.*"}[{{ .Values.ingress5xxErrorWindowMinutesHealthEndpoint }}m])) > {{ .Values.ingress5xxErrorInstances }}
            {{ $businessOrAllHoursExpression }}
          for: 5m
          labels:
            severity: {{ .Values.alertSeverity }}

        - alert: RatelimitBlocking
          annotations:
            message: Rate limit is being applied on ingress {{`{{`}} $labels.exported_namespace {{`}}`}}/{{`{{`}} $labels.ingress {{`}}`}}.
            runbook_url: {{ .Values.runbookUrl }}ingress-rate-limiting
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |-
            avg(rate(nginx_ingress_controller_requests{exported_namespace=~"{{ $targetNamespace }}", ingress=~"{{ $targetIngress }}", status="429"}[1m]) > 0) by (ingress, exported_namespace)
            {{ $businessOrAllHoursExpression }}
          for: 1m
          labels:
            severity: {{ .Values.alertSeverity }}
        - alert: ModSecurityBlocking
          annotations:
            message: Mod_Security is blocking ingress {{`{{`}} $labels.exported_namespace {{`}}`}}/{{`{{`}} $labels.ingress {{`}}`}}. Blocking http requests at rate of {{`{{`}} printf "%.2f" $value {{`}}`}} per second.
            runbook_url: {{ .Values.runbookUrl }}ingress-modsecurity-blocking
            dashboard_url: {{ $.Values.grafanaUrl }}/d/golden-signals/golden-signals?orgId=1&var-namespace={{ $targetNamespace }}&var-service={{ $targetApplication }}
          expr: |-
            avg(rate(nginx_ingress_controller_requests{exported_namespace=~"{{ $targetNamespace }}", ingress=~"{{ $targetIngress }}", status="{{ $modSecBlockingStatusCode }}"}[1m]) > 0.33) by (ingress, exported_namespace)
            {{ $businessOrAllHoursExpression }}
          for: 1m
          labels:
            severity: {{ .Values.alertSeverity }}
{{- end }}
