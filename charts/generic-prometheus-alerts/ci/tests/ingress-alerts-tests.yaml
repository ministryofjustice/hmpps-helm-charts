---
rule_files:
  - ../test-application-ingress.yaml
  - ../test-business-hours-ingress.yaml

evaluation_interval: 1m

tests:
  - interval: 1m
    input_series:
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", path="/bob", status="500"}'
        values: '0 1 3'
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", path="/joe", status="501"}'
        values: '0 2 5'

    name: 5xxErrorResponses alert fired as request rate increases over the duration of the 1 minute time range window
  
    promql_expr_test:
      - expr: sum(increase(nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", path!="/health", status=~"5.*"}[5m]) > 0) by (ingress, exported_namespace)
        eval_time: 2m
        exp_samples:
          - labels: '{exported_namespace="test-application-dev", ingress="test-application-ingress"}'
            value: 8
    alert_rule_test:
      # assert that the 5xx alert is firing
      - eval_time: 2m
        alertname: 5xxErrorResponses
        exp_alerts:
          - exp_labels:
              alertname: 5xxErrorResponses
              exported_namespace: test-application-dev
              ingress: test-application-ingress
              severity: alert-severity-tag
            exp_annotations:
              message: "Ingress test-application-dev/test-application-ingress is serving 5xx responses."
              runbook_url: https://runbook.com/ingress-5xx-error-responses
              dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/f1e13059dfd23fdcaf479f4fa833f92610c2dfa5/kubernetes-ingress-traffic?1&var-namespace=test-application-dev&var-ingress=test-application.*&from=now-5m&to=now
      # and that the 5xx health alert is not firing
      - eval_time: 2m
        alertname: 5xxErrorResponsesOnHealthEndpoint
        exp_alerts: []

  - interval: 1m
    input_series:
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", path="/health", status="500"}'
        values: '0 1 3'
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", path="/health", status="501"}'
        values: '0 6 5'

    name: 5xxErrorResponses alert not fired as path is for /health

    promql_expr_test:
      - expr: sum(increase(nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", path!="/health", status=~"5.*"}[5m]) > 0) by (ingress, exported_namespace)
        eval_time: 6m
        exp_samples: []
      - expr: sum(increase(nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", path="/health", status=~"5.*"}[5m]) > 0) by (ingress, exported_namespace)
        eval_time: 6m
        exp_samples:
          - labels: '{exported_namespace="test-application-dev", ingress="test-application-ingress"}'
            value: 10.5

    alert_rule_test:
      # assert that the 5xx alert is not firing
      - eval_time: 6m
        alertname: 5xxErrorResponses
        exp_alerts: []
      # and that the 5xx health alert is firing
      - eval_time: 6m
        alertname: 5xxErrorResponsesOnHealthEndpoint
        exp_alerts:
          - exp_labels:
              alertname: 5xxErrorResponsesOnHealthEndpoint
              exported_namespace: test-application-dev
              ingress: test-application-ingress
              severity: alert-severity-tag
            exp_annotations:
              message: "High rate of 5xx errors detected on /health path in Ingress test-application-dev/test-application-ingress."
              runbook_url: https://runbook.com/ingress-5xx-error-responses
              dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/f1e13059dfd23fdcaf479f4fa833f92610c2dfa5/kubernetes-ingress-traffic?1&var-namespace=test-application-dev&var-ingress=test-application.*&from=now-5m&to=now

  - interval: 1m
    input_series:
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-business-hours-dev", ingress="test-business-hours-ingress", path="/bob", status="500"}'
        values: '0 1 3'
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-business-hours-dev", ingress="test-business-hours-ingress", path="/joe", status="501"}'
        values: '0 2 5'

    name: 5xxErrorResponses alert doesn't fire outside of business hours

    promql_expr_test:
      - expr: avg(rate(nginx_ingress_controller_requests{exported_namespace="test-business-hours-dev", ingress="test-business-hours-ingress", path!="/health", status=~"5.*"}[1m]) > 0) by (ingress, exported_namespace)
        eval_time: 2m
        exp_samples:
          - labels: '{exported_namespace="test-business-hours-dev", ingress="test-business-hours-ingress"}'
            value: 0.04166666666666667

    alert_rule_test:
      # assert that the 5xx alert is not firing as outside of business hours
      - eval_time: 2m
        alertname: 5xxErrorResponses
        exp_alerts: []

  - interval: 1m
    input_series:
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", path="/bob", status="500"}'
        values: '0 1 0'

    name: 5xxErrorResponses alert not fired as request rate decreases over the duration of the 1 minute time range window

    # this is here to help folks better understand the promql expression
    # at 2m the rate (increase per second) in the last minute: 0 which is not greater than 0, assert nothing is returned
    promql_expr_test:
      - expr: avg(rate(nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", path!="/health", status=~"5.*"}[1m]) > 0) by (ingress, exported_namespace)
        eval_time: 2m
        exp_samples: []

    alert_rule_test:
      # assert that the 5xx alert is NOT firing
      - eval_time: 2m
        alertname: 5xxErrorResponses
        exp_alerts: []

  ## alert:
  ##   - RatelimitBlocking
  ##
  - interval: 1m
    input_series:
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", status="429"}'
        values: '0 1 3'

    name: RatelimitBlocking alert fired as request rate increases over the duration of the 1 minute time range window

    # this is here to help folks better understand the promql expression
    # assert that
    #   at 2m the rate (increase per second) in the last minute: 0.03 ((3 - 1) / 60)
    #   which is greater than 0
    #   the average is 0.03
    promql_expr_test:
      - expr: avg(rate(nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", status=~"429"}[1m]) > 0) by (ingress, exported_namespace)
        eval_time: 2m
        exp_samples:
          - labels: '{exported_namespace="test-application-dev", ingress="test-application-ingress"}'
            value: 0.03333333333333333

    alert_rule_test:
      # assert that the RatelimitBlocking alert is firing
      - eval_time: 2m
        alertname: RatelimitBlocking
        exp_alerts:
          - exp_labels:
              alertname: RatelimitBlocking
              exported_namespace: test-application-dev
              ingress: test-application-ingress
              severity: alert-severity-tag
            exp_annotations:
              message: "Rate limit is being applied on ingress test-application-dev/test-application-ingress."
              runbook_url: https://runbook.com/ingress-rate-limiting
              dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/golden-signals/golden-signals?orgId=1&var-namespace=test-application-dev&var-service=test-application

  - interval: 1m
    input_series:
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", status="429"}'
        values: '0 1 0'

    name: RatelimitBlocking alert not fired as request rate decreases over the duration of the 1 minute time range window

    # this is here to help folks better understand the promql expression
    # at 2m the rate (increase per second) in the last minute: 0 which is not greater than 0, assert nothing is returned
    promql_expr_test:
      - expr: avg(rate(nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", status=~"429"}[1m]) > 0) by (ingress, exported_namespace)
        eval_time: 2m
        exp_samples: []

    alert_rule_test:
      # assert that the RatelimitBlocking alert is NOT firing
      - eval_time: 2m
        alertname: RatelimitBlocking
        exp_alerts: []

  ## alert:
  ##   - ModSecurityBlocking
  ##
  - interval: 1m
    input_series:
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", status="406"}'
        values: '0 21 43'

    name: ModSecurityBlocking alert fired as request rate is greater than 20 over the duration of the 1 minute time range window

    # this is here to help folks better understand the promql expression
    # assert that
    #   at 2m the rate (increase per second) in the last minute: 0.35 ((43 - 21) / 60)
    #   which is greater than 0.33
    #   the average is 0.36
    promql_expr_test:
      - expr: avg(rate(nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", status=~"406"}[1m]) > 0.33) by (ingress, exported_namespace)
        eval_time: 2m
        exp_samples:
          - labels: '{exported_namespace="test-application-dev", ingress="test-application-ingress"}'
            value: 0.36666666666666664

    alert_rule_test:
      # assert that the ModSecurityBlocking alert is firing
      - eval_time: 2m
        alertname: ModSecurityBlocking
        exp_alerts:
          - exp_labels:
              alertname: ModSecurityBlocking
              exported_namespace: test-application-dev
              ingress: test-application-ingress
              severity: alert-severity-tag
            exp_annotations:
              message: "Mod_Security is blocking ingress test-application-dev/test-application-ingress. Blocking http requests at rate of 0.37 per second."
              runbook_url: https://runbook.com/ingress-modsecurity-blocking
              dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/golden-signals/golden-signals?orgId=1&var-namespace=test-application-dev&var-service=test-application

  - interval: 1m
    input_series:
      - series: 'nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", status="406"}'
        values: '0 20 38'

    name: ModSecurityBlocking alert not fired as request rate is equal to the threshold over the duration of the 1 minute time range window

    # this is here to help folks better understand the promql expression
    # at 2m the rate (increase per second) in the last minute: 0.3 ((38 - 20) / 60) which not greater than 0.33, assert nothing is returned
    promql_expr_test:
      - expr: avg(rate(nginx_ingress_controller_requests{exported_namespace="test-application-dev", ingress="test-application-ingress", status=~"406"}[1m]) > 0.33) by (ingress, exported_namespace)
        eval_time: 2m
        exp_samples: []

    alert_rule_test:
      # assert that the ModSecurityBlocking alert is NOT firing
      - eval_time: 2m
        alertname: ModSecurityBlocking
        exp_alerts: []
