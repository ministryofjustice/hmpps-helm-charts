---
# Default values for generic-prometheus-alerts.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Name of application, which is used to filter the alert queries, override specific resource types below.
targetApplication: ""

# To disable inclusion of ingress alerts, set to false, defaults true
ingressAlertsEnabled: true

# The alert severity label is used within Alert Manager and determines which slack channel alerts are sent to.
alertSeverity: ""

businessUnit: hmpps

runbookUrl: https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#
grafanaUrl: https://grafana.live.cloud-platform.service.justice.gov.uk
dashboardId: f1e13059dfd23fdcaf479f4fa833f92610c2dfa5
orgId: 1

# Alert by default will target resources uses a regex and the application name (.Values.targetApplication).
# To be more or less specific with promql queries set these overrides in the parent chart, examples:

# podTargetOverride: ".*"

# deploymentTargetOverride: ".*"

# ingressTargetOverride: ".*"

# hpaTargetOverride: ".*"

# Mod_Security normally blocks requests with http status 403 - forbidden.
# To be able to distinguish and specifically alert on mod_security blocking
# the general pattern has been to update the mod_security config to block
# using a different status code. The default is 406 but this can be overridden:Â 

# modSecBlockingStatusCodeOverride: "406"

extraAnnotations: {}

extraLabels:
  prometheus: cloud-platform

additionalRuleLabels: {}

# To monitor AWS SQS queues with oldest message and number of messages alerts, either add the name of the queue in a list like this:
# sqsAlertsQueueNames:
#   - "queue-name"
#   - "queue-name-2"
# this will add the queues listed to both alerts. If you want to supply different queues to the alerts use:
# sqsOldestAlertQueueNames:
#   - "queue-name"
#   - "queue-name-2"
# sqsNumberAlertQueueNames:
#   - "other-queue-name"
#   - "other-queue-name-2"
#
# If you have lots of AWS SQS then these can be grouped so that there are separate alerts for each group rather than
# one prometheus alert to cover them all. At present these alerts are only created for the number of messages.
# sqsNumberAlertQueueMappings:
#   group-one:
#     - "name-one-queue"
#     - "name-one-dlq"
#   group-two:
#     - "name-two-queue"
#     - "name-two-dlq"

# Set the alert threshold for SQS oldest message in the queue (minutes).
# sqsAlertsOldestThreshold: 30

# Set the alert threshold for total number of SQS on the queue waiting to be processed.
# sqsAlertsTotalMessagesThreshold: 100

# To monitor AWS SNS Topic queues, add the topic names in a list.
# snsAlertsTopicNames:
#   topic-name-id: "Topic nice display name"
#   topic-name-2-id: "Topic nice display name 2"

# Set the alert threshold for no SNS messages published to topic (minutes).
# snsAlertsNoMessagesThresholdMinutes: 30

# To monitor AWS Elasticache, add the cluster node names in a list as follows.
# elastiCacheAlertsClusterIds:
#   elasticache-cluster-1-001: "Meaningful cluster node display name"
#   elasticache-cluster-1-002: "Meaningful cluster node display name 2"

# Maxiumum threshold for Elasticache EngineCPUUtilisation - value between 0 and 100.
elastiCacheAlertsEngineCPUThreshold: 75

# Duration over which to measure Elasticache EngineCPUUtilisation (minutes).
elastiCacheAlertsEngineCPUThresholdMinutes: 5

# Maximum threshold for Elasticache CPUUtilisation - value between 0 and 100.
elastiCacheAlertsCPUThreshold: 75

# Duration over which to measure Elasticache CPUUtilisation (minutes).
elastiCacheAlertsCPUThresholdMinutes: 5

# Minimum threshold for Elasticache FreeMemory - value should be above 150MB.
elastiCacheAlertsFreeMemoryThreshold: 150

# Duration over which to measure Elasticache FreeMemory (minutes).
elastiCacheAlertsFreeMemoryThresholdMinutes: 5

# Maximum threshold for Elasticache DatabaseMemoryUsagePercentage - value between 0 and 100.
elastiCacheAlertsMemoryUsageThreshold: 75

# Duration over which to measure Elasticache DatabaseMemoryUsagePercentage (minutes).
elastiCacheAlertsMemoryUsageThresholdMinutes: 5

# To monitor AWS RDS instances, add the name of the rds instance in a list.
# This can be obtained from the instance address in the rds secret in the namespace
# and should be the first part of the address e.g. cloud-platform-13962204fd879828
# rdsAlertsDatabases:
#   database-name-id: "Database nice display name"
#   database-name-2-id: "Database nice display name 2"

# Set the alert CPU threshold - value between 0 and 100.
rdsAlertsCPUThreshold: 80

# Set the alert CPU threshold (minutes).
rdsAlertsCPUThresholdMinutes: 5

# Set the alert number of connections threshold.
# AWS cloudwatch doesn't expose the maximum number of connections, so this needs to be calculated.
# https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html contains information on how
# much memory each instance class has e.g. db.t3.small is 2Gib, db.t3.medium is 4GiB.
# https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Limits.html#RDS_Limits.MaxConnections then provides
# calculations for each database on the maximum number of database connections so for a db.t3.small database it will be
# LEAST({2Gib/9531392}, 5000) = LEAST({2 * 1024 * 1024 * 1024/9531392}, 5000) = LEAST(225, 5000) = 225.
rdsAlertsConnectionThreshold: 180

# Set the alert number of connections threshold (minutes).
rdsAlertsConnectionThresholdMinutes: 5

# Set the error window minutes for 5xx ingress
ingress5xxErrorWindowMinutes: 1

# Set the error window minutes for 5xxErrorResponsesOnHealthEndpoint
ingress5xxErrorWindowMinutesHealthEndpoint: 5

# Set the threshold for 5xx ingress for 5xxErrorResponsesOnHealthEndpoint
# Default of 0.004 chosen as for hmpps-prisoner-to-nomis-update-prod we often get false positives around the 0.00375
# mark, so for our app going slightly higher than that stops the alerts.
ingress5xxHealthEndpointThreshold: 0.004

# Whether to enable checks for successful ingress responses.
# Defaults to false (disabled) since services will not get consistent traffic in dev and pre-prod and might not get
# consistent traffic in production either.
ingress2xxEnabled: false

# Set the window minutes for 2xx ingress responses
ingress2xxWindowMinutes: 60

# Set the minimum amount of 2xx ingress responses in ingress2xxWindowMinutes time window.
# This excludes any requests to /health and internal requests to /ping, but includes requests to /info.
ingress2xxThreshold: 100

# Alert for business hours only - 8am to 6pm on week days (Monday to Friday)
businessHoursOnly: false

# Set the time frame in minutes for CronJobStatusFailed alert
# The job used to check whether the status was failure and alert at that specific moment, however in the period when
# the job runs the failure flag is not set which caused the alert to then be resolved.  The time window should be set
# to a duration longer than the duration of the job so that the alert doesn't clear (if failing) during run of the job.
applicationCronJobStatusFailedWindowMinutes: 5
